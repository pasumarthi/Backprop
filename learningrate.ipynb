{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learningrate.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasumarthi/Backprop/blob/master/learningrate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J42_-apg4DaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "# Code is ported from https://github.com/fastai/fastai\n",
        "class OneCycleLR(Callback):\n",
        "    def __init__(self,\n",
        "                 max_lr,\n",
        "                 end_percentage=0.1,\n",
        "                 scale_percentage=None,\n",
        "                 maximum_momentum=0.95,\n",
        "                 minimum_momentum=0.85,\n",
        "                 verbose=True):\n",
        "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
        "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
        "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
        "        100th its initial lowest value.\n",
        "        # Arguments:\n",
        "            max_lr: Float. Initial learning rate. This also sets the\n",
        "                starting learning rate (which will be 10x smaller than\n",
        "                this), and will increase to this value during the first cycle.\n",
        "            end_percentage: Float. The percentage of all the epochs of training\n",
        "                that will be dedicated to sharply decreasing the learning\n",
        "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
        "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
        "                If None, it will compute the scale_percentage automatically\n",
        "                based on the `end_percentage`.\n",
        "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
        "                value, which gradually drops to its lowest value in half-cycle,\n",
        "                then gradually increases again to stay constant at this max value.\n",
        "                Can only be used with SGD Optimizer.\n",
        "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
        "                the half-cycle. Can only be used with SGD Optimizer.\n",
        "            verbose: Bool. Whether to print the current learning rate after every\n",
        "                epoch.\n",
        "        # Reference\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
        "        \"\"\"\n",
        "        super(OneCycleLR, self).__init__()\n",
        "\n",
        "        if end_percentage < 0. or end_percentage > 1.:\n",
        "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
        "\n",
        "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
        "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
        "\n",
        "        self.initial_lr = max_lr\n",
        "        self.end_percentage = end_percentage\n",
        "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
        "        self.max_momentum = maximum_momentum\n",
        "        self.min_momentum = minimum_momentum\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.max_momentum is not None and self.min_momentum is not None:\n",
        "            self._update_momentum = True\n",
        "        else:\n",
        "            self._update_momentum = False\n",
        "\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self.epochs = None\n",
        "        self.batch_size = None\n",
        "        self.samples = None\n",
        "        self.steps = None\n",
        "        self.num_iterations = None\n",
        "        self.mid_cycle_id = None\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the callback.\n",
        "        \"\"\"\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "    def compute_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the learning rate based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the learning rate gradually increases.\n",
        "        - If in the second half of training, the learning rate gradually decreases.\n",
        "        - If in the final `end_percentage` portion of training, the learning rate\n",
        "            is quickly reduced to near 100th of the original min learning rate.\n",
        "        # Returns:\n",
        "            the new learning rate\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
        "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
        "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
        "                                              (1. - 100.) / 100.)) * self.scale\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - (\n",
        "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        if self.clr_iterations == self.num_iterations:\n",
        "            self.clr_iterations = 0\n",
        "\n",
        "        return new_lr\n",
        "\n",
        "    def compute_momentum(self):\n",
        "        \"\"\"\n",
        "         Compute the momentum based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the momentum gradually decreases.\n",
        "        - If in the second half of training, the momentum gradually increases.\n",
        "        - If in the final `end_percentage` portion of training, the momentum value\n",
        "            is kept constant at the maximum initial value.\n",
        "        # Returns:\n",
        "            the new momentum value\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            new_momentum = self.max_momentum\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
        "                                        self.mid_cycle_id))\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        return new_momentum\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.epochs = self.params['epochs']\n",
        "        self.batch_size = self.params['batch_size']\n",
        "        self.samples = self.params['samples']\n",
        "        self.steps = self.params['steps']\n",
        "\n",
        "        if self.steps is not None:\n",
        "            self.num_iterations = self.epochs * self.steps\n",
        "        else:\n",
        "            if (self.samples % self.batch_size) == 0:\n",
        "                remainder = 0\n",
        "            else:\n",
        "                remainder = 1\n",
        "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
        "\n",
        "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
        "\n",
        "        self._reset()\n",
        "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        new_lr = self.compute_lr()\n",
        "\n",
        "        self.history.setdefault('lr', []).append(\n",
        "            K.get_value(self.model.optimizer.lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "\n",
        "            self.history.setdefault('momentum', []).append(\n",
        "                K.get_value(self.model.optimizer.momentum))\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.verbose:\n",
        "            if self._update_momentum:\n",
        "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
        "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
        "\n",
        "            else:\n",
        "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
        "\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    def __init__(self,\n",
        "                 num_samples,\n",
        "                 batch_size,\n",
        "                 minimum_lr=1e-5,\n",
        "                 maximum_lr=10.,\n",
        "                 lr_scale='exp',\n",
        "                 validation_data=None,\n",
        "                 validation_sample_rate=5,\n",
        "                 stopping_criterion_factor=4.,\n",
        "                 loss_smoothing_beta=0.98,\n",
        "                 save_dir=None,\n",
        "                 verbose=True):\n",
        "        \"\"\"\n",
        "        This class uses the Cyclic Learning Rate history to find a\n",
        "        set of learning rates that can be good initializations for the\n",
        "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
        "        below.\n",
        "        A port of the Fast.ai implementation for Keras.\n",
        "        # Note\n",
        "        This requires that the model be trained for exactly 1 epoch. If the model\n",
        "        is trained for more epochs, then the metric calculations are only done for\n",
        "        the first epoch.\n",
        "        # Interpretation\n",
        "        Upon visualizing the loss plot, check where the loss starts to increase\n",
        "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
        "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
        "        Choose the max value as this value when passing the `max_val` argument\n",
        "        to OneCycleLR callback.\n",
        "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
        "        # Arguments:\n",
        "            num_samples: Integer. Number of samples in the dataset.\n",
        "            batch_size: Integer. Batch size during training.\n",
        "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
        "            maximum_lr: Float. Final learning rate (and the maximum).\n",
        "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
        "                scaling for each update to the learning rate during subsequent\n",
        "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
        "            validation_data: Requires the validation dataset as a tuple of\n",
        "                (X, y) belonging to the validation set. If provided, will use the\n",
        "                validation set to compute the loss metrics. Else uses the training\n",
        "                batch loss. Will warn if not provided to alert the user.\n",
        "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
        "                validation set per iteration of the LRFinder. Larger number of\n",
        "                samples will reduce the variance but will take longer time to execute\n",
        "                per batch.\n",
        "                If Positive > 0, will sample from the validation dataset\n",
        "                If Megative, will use the entire dataset\n",
        "            stopping_criterion_factor: Integer or None. A factor which is used\n",
        "                to measure large increase in the loss value during training.\n",
        "                Since callbacks cannot stop training of a model, it will simply\n",
        "                stop logging the additional values from the epochs after this\n",
        "                stopping criterion has been met.\n",
        "                If None, this check will not be performed.\n",
        "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
        "                average of the loss function.\n",
        "            save_dir: Optional, String. If passed a directory path, the callback\n",
        "                will save the running loss and learning rates to two separate numpy\n",
        "                arrays inside this directory. If the directory in this path does not\n",
        "                exist, they will be created.\n",
        "            verbose: Whether to print the learning rate after every batch of training.\n",
        "        # References:\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "        \"\"\"\n",
        "        super(LRFinder, self).__init__()\n",
        "\n",
        "        if lr_scale not in ['exp', 'linear']:\n",
        "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
        "\n",
        "        if validation_data is not None:\n",
        "            self.validation_data = validation_data\n",
        "            self.use_validation_set = True\n",
        "\n",
        "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
        "                self.validation_sample_rate = validation_sample_rate\n",
        "            else:\n",
        "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
        "        else:\n",
        "            self.use_validation_set = False\n",
        "            self.validation_sample_rate = 0\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "        self.initial_lr = minimum_lr\n",
        "        self.final_lr = maximum_lr\n",
        "        self.lr_scale = lr_scale\n",
        "        self.stopping_criterion_factor = stopping_criterion_factor\n",
        "        self.loss_smoothing_beta = loss_smoothing_beta\n",
        "        self.save_dir = save_dir\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.num_batches_ = num_samples // batch_size\n",
        "        self.current_lr_ = minimum_lr\n",
        "\n",
        "        if lr_scale == 'exp':\n",
        "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
        "                1. / float(self.num_batches_))\n",
        "        else:\n",
        "            extra_batch = int((num_samples % batch_size) != 0)\n",
        "            self.lr_multiplier_ = np.linspace(\n",
        "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
        "\n",
        "        # If negative, use entire validation set\n",
        "        if self.validation_sample_rate < 0:\n",
        "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
        "\n",
        "        self.current_batch_ = 0\n",
        "        self.current_epoch_ = 0\n",
        "        self.best_loss_ = 1e6\n",
        "        self.running_loss_ = 0.\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "\n",
        "        self.current_epoch_ = 1\n",
        "        K.set_value(self.model.optimizer.lr, self.initial_lr)\n",
        "\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.current_batch_ = 0\n",
        "\n",
        "        if self.current_epoch_ > 1:\n",
        "            warnings.warn(\n",
        "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
        "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        self.current_batch_ += 1\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        if self.current_epoch_ > 1:\n",
        "            return\n",
        "\n",
        "        if self.use_validation_set:\n",
        "            X, Y = self.validation_data[0], self.validation_data[1]\n",
        "\n",
        "            # use 5 random batches from test set for fast approximate of loss\n",
        "            num_samples = self.batch_size * self.validation_sample_rate\n",
        "\n",
        "            if num_samples > X.shape[0]:\n",
        "                num_samples = X.shape[0]\n",
        "\n",
        "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
        "            x = X[idx]\n",
        "            y = Y[idx]\n",
        "\n",
        "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
        "            loss = values[0]\n",
        "        else:\n",
        "            loss = logs['loss']\n",
        "\n",
        "        # smooth the loss value and bias correct\n",
        "        running_loss = self.loss_smoothing_beta * loss + (\n",
        "            1. - self.loss_smoothing_beta) * loss\n",
        "        running_loss = running_loss / (\n",
        "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
        "\n",
        "        # stop logging if loss is too large\n",
        "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
        "                running_loss >\n",
        "                self.stopping_criterion_factor * self.best_loss_):\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
        "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
        "            return\n",
        "\n",
        "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
        "            self.best_loss_ = running_loss\n",
        "\n",
        "        current_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
        "        if self.lr_scale == 'exp':\n",
        "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
        "        else:\n",
        "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
        "\n",
        "        # compute the lr for the next batch and update the optimizer lr\n",
        "        if self.lr_scale == 'exp':\n",
        "            current_lr *= self.lr_multiplier_\n",
        "        else:\n",
        "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
        "\n",
        "        K.set_value(self.model.optimizer.lr, current_lr)\n",
        "\n",
        "        # save the other metrics as well\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        if self.verbose:\n",
        "            if self.use_validation_set:\n",
        "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
        "                      (values[0], current_lr))\n",
        "            else:\n",
        "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
        "            if not os.path.exists(self.save_dir):\n",
        "                os.makedirs(self.save_dir)\n",
        "\n",
        "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
        "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
        "\n",
        "            np.save(losses_path, self.losses)\n",
        "            np.save(lrs_path, self.lrs)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
        "                      % (self.save_dir))\n",
        "\n",
        "        self.current_epoch_ += 1\n",
        "\n",
        "        warnings.simplefilter(\"default\")\n",
        "\n",
        "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the callback itself.\n",
        "        # Arguments:\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\n",
        "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses = self.losses\n",
        "        lrs = self.lrs\n",
        "\n",
        "        if clip_beginning:\n",
        "            losses = losses[clip_beginning:]\n",
        "            lrs = lrs[clip_beginning:]\n",
        "\n",
        "        if clip_endding:\n",
        "            losses = losses[:clip_endding]\n",
        "            lrs = lrs[:clip_endding]\n",
        "\n",
        "        plt.plot(lrs, losses)\n",
        "        plt.title('Learning rate vs Loss')\n",
        "        plt.xlabel('learning rate')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "    @classmethod\n",
        "    def restore_schedule_from_dir(cls,\n",
        "                                  directory,\n",
        "                                  clip_beginning=None,\n",
        "                                  clip_endding=None):\n",
        "        \"\"\"\n",
        "        Loads the training history from the saved numpy files in the given directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        Returns:\n",
        "            tuple of (losses, learning rates)\n",
        "        \"\"\"\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses_path = os.path.join(directory, 'losses.npy')\n",
        "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
        "\n",
        "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
        "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
        "                  (losses_path, lrs_path, directory))\n",
        "\n",
        "            losses = None\n",
        "            lrs = None\n",
        "\n",
        "        else:\n",
        "            losses = np.load(losses_path)\n",
        "            lrs = np.load(lrs_path)\n",
        "\n",
        "            if clip_beginning:\n",
        "                losses = losses[clip_beginning:]\n",
        "                lrs = lrs[clip_beginning:]\n",
        "\n",
        "            if clip_endding:\n",
        "                losses = losses[:clip_endding]\n",
        "                lrs = lrs[:clip_endding]\n",
        "\n",
        "        return losses, lrs\n",
        "\n",
        "    @classmethod\n",
        "    def plot_schedule_from_file(cls,\n",
        "                                directory,\n",
        "                                clip_beginning=None,\n",
        "                                clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
        "        rate values in the specified directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
        "            return\n",
        "\n",
        "        losses, lrs = cls.restore_schedule_from_dir(\n",
        "            directory,\n",
        "            clip_beginning=clip_beginning,\n",
        "            clip_endding=clip_endding)\n",
        "\n",
        "        if losses is None or lrs is None:\n",
        "            return\n",
        "        else:\n",
        "            plt.plot(lrs, losses)\n",
        "            plt.title('Learning rate vs Loss')\n",
        "            plt.xlabel('learning rate')\n",
        "            plt.ylabel('loss')\n",
        "            plt.show()\n",
        "\n",
        "    @property\n",
        "    def lrs(self):\n",
        "        return np.array(self.history['log_lrs'])\n",
        "\n",
        "    @property\n",
        "    def losses(self):\n",
        "        return np.array(self.history['running_loss_'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}